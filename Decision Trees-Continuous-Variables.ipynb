{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "sb.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SepalLength</th>\n",
       "      <th>SepalWidth</th>\n",
       "      <th>PetalLength</th>\n",
       "      <th>PetalWidth</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SepalLength  SepalWidth  PetalLength  PetalWidth        class\n",
       "0          4.9         3.0          1.4         0.2  Iris-setosa\n",
       "1          4.7         3.2          1.3         0.2  Iris-setosa\n",
       "2          4.6         3.1          1.5         0.2  Iris-setosa\n",
       "3          5.0         3.6          1.4         0.2  Iris-setosa\n",
       "4          5.4         3.9          1.7         0.4  Iris-setosa"
      ]
     },
     "execution_count": 614,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "org_data = pd.read_csv(\"./iris.data\")\n",
    "#print(org_data)\n",
    "\n",
    "#assigning names to each column (features and label)\n",
    "org_data.columns=['SepalLength','SepalWidth','PetalLength','PetalWidth','class']\n",
    "#print(org_data)\n",
    "\n",
    "#printing first 5 rows in beautiful tabular form \n",
    "org_data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting data in to training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting data where split_size is in % (i.e. 70%) ==> it will then divide data in to 70% train_data and 30% test_data\n",
    "def split_data (org_data, split_size):\n",
    "    \n",
    "    org_data_size = len(org_data)\n",
    "    #print (org_data_size)\n",
    "    test_data_size = round((split_size/100)*org_data_size)\n",
    "    train_data_size = round(org_data_size-((split_size/100)*org_data_size))\n",
    "    #print (train_data_size, test_data_size)\n",
    "\n",
    "    #creating list of indexes which will help to randomly select datapoints from the whole data to split dataset\n",
    "    indexes_list = org_data.index.tolist()\n",
    "    #print(indices_list)\n",
    "\n",
    "    #randomly selecting test datapoint's indexes\n",
    "    test_data_indexes = random.sample (population=indexes_list, k= test_data_size)\n",
    "\n",
    "    #print(test_data_indexes)\n",
    "\n",
    "    #using loc to access rows of data frames for specific indexes and storing it in testing data    \n",
    "    testing_data = org_data.loc[test_data_indexes]\n",
    "    \n",
    "    #testing data is being dropped and rest data is getting stored in training data\n",
    "    training_data = org_data.drop(test_data_indexes)\n",
    "\n",
    "    #print (type(testing_data),type(training_data))\n",
    "    \n",
    "    return training_data, testing_data\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to generate same random numbers every time\n",
    "random.seed(0)\n",
    "\n",
    "#splitting data into 70% training and 30% testing data\n",
    "training_data, testing_data = split_data(org_data, split_size=30)\n",
    "#print (training_data)\n",
    "#print (testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.9, 3.0, 1.4, 0.2, 'Iris-setosa'],\n",
       "       [4.6, 3.1, 1.5, 0.2, 'Iris-setosa'],\n",
       "       [5.0, 3.6, 1.4, 0.2, 'Iris-setosa'],\n",
       "       [5.4, 3.9, 1.7, 0.4, 'Iris-setosa'],\n",
       "       [4.6, 3.4, 1.4, 0.3, 'Iris-setosa']], dtype=object)"
      ]
     },
     "execution_count": 617,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(training_data.values)\n",
    "\n",
    "# getting training data as 2-d list\n",
    "data = training_data.values\n",
    "\n",
    "# printing first 10 rows\n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking singularity of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking if the data has only one class, so it automatically belongs to that classes and reaches leaf.\n",
    "def check_singularity(data):\n",
    "    \n",
    "    # extracting only class column of the dataset\n",
    "    classes = data[:,-1]\n",
    "    \n",
    "    #extracting unique classes\n",
    "    unique_classes = np.unique(classes)\n",
    "    #print(unique_classes)\n",
    "    \n",
    "    #if there is only one class return true else false\n",
    "    if (len(unique_classes) == 1):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check_singularity(training_data.values)\n",
    "\n",
    "def classify_data (data): \n",
    "    # extracting only class column of the dataset\n",
    "    classes = data[:,-1]\n",
    "    \n",
    "    #extracting unique classes and its count\n",
    "    unique_classes, unique_classes_count = np.unique(classes,return_counts=True)\n",
    "    \n",
    "    # storing max unique count's index\n",
    "    class_max_count_index = unique_classes_count.argmax()\n",
    "    \n",
    "    # storing class with max count\n",
    "    class_classified = unique_classes[class_max_count_index]\n",
    "    \n",
    "    return class_classified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#function to get all the splits between the data points\n",
    "def get_splits(data):\n",
    "    #making dictionary for splits\n",
    "    splits = {}\n",
    "    # no of columns to extract features for splitting the data\n",
    "    no_of_columns = len(data[0])\n",
    "    \n",
    "    #column-1 because we are not interested in class/label\n",
    "    for column in range (0,no_of_columns-1):\n",
    "        #for every column storing splits\n",
    "        splits [column] = []\n",
    "        \n",
    "        #extracting data column wise\n",
    "        specific_column_data = data[:,column]\n",
    "        \n",
    "        #getting unique values in the columns\n",
    "        unique_specific_column_data = np.unique(specific_column_data)\n",
    "        \n",
    "        #starting loop from 1 because we will get previous class also and index 0 has no previous element\n",
    "        for index in range (1,len(unique_specific_column_data)):\n",
    "            current_column_value = unique_specific_column_data [index]\n",
    "            previous_column_value = unique_specific_column_data[index-1]\n",
    "            \n",
    "            #split between two data points\n",
    "            split = (current_column_value+previous_column_value)/2\n",
    "            \n",
    "            #storing split in respective column's array in dictionary\n",
    "            splits[column].append(split)\n",
    "            \n",
    "            \n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to split the data given split column and specific value in that column\n",
    "def split_data(data, split_column, split_value):\n",
    "    \n",
    "    #extracting split column data values\n",
    "    split_column_data = data[:, split_column]\n",
    "\n",
    "    #data below and above that split\n",
    "    data_below = data[split_column_data <= split_value]\n",
    "    data_above = data[split_column_data >  split_value]\n",
    "    \n",
    "    return data_below, data_above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy Measurement to Determine the best split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {},
   "outputs": [],
   "source": [
    "# measure entropy of different splits \n",
    "def measure_entropy(data):\n",
    "    \n",
    "    #extracting last column\n",
    "    class_column = data[:, -1]\n",
    "    #extracting unique column's counts\n",
    "    _, unique_class_counts = np.unique(class_column, return_counts=True)\n",
    "\n",
    "    #calculating probabilities of each class\n",
    "    probabilities = unique_class_counts / unique_class_counts.sum()\n",
    "    \n",
    "    # weighted sum of product of probalility and uncertainity\n",
    "    entropy = sum(probabilities * -np.log2(probabilities))\n",
    "     \n",
    "    #print (entropy)    \n",
    "    return entropy\n",
    "\n",
    "#measure_entropy(training_data.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating overall entropy (data below + data above)\n",
    "def measure_overall_entropy(data_below, data_above):\n",
    "    \n",
    "    n = len(data_below) + len(data_above)\n",
    "    p_data_below = len(data_below) / n\n",
    "    p_data_above = len(data_above) / n\n",
    "\n",
    "    #first calculating entropy for data below then data above and then summing to get overall entropy which will be used to \n",
    "    # determine the best split\n",
    "    overall_entropy =  (p_data_below * measure_entropy(data_below) \n",
    "                      + p_data_above * measure_entropy(data_above))\n",
    "    \n",
    "\n",
    "    return overall_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to get the best split\n",
    "def find_best_split(data, splits):\n",
    "    \n",
    "    #first setting entropy to max but considering it as minimum entropy\n",
    "    min_entropy = 99999\n",
    "    \n",
    "    #looping the dataframe having lists\n",
    "    for column in splits:\n",
    "        #loop for the list inside dataframe\n",
    "        for value in splits[column]:\n",
    "            #extracting data below the split and data above\n",
    "            data_below, data_above = split_data (data, split_column=column, split_value=value)\n",
    "            #calculating overall entropy for that split\n",
    "            entropy = measure_overall_entropy(data_below, data_above)\n",
    "            \n",
    "            #check to determine minimum entropy which will determine the best split\n",
    "            if entropy <= min_entropy:\n",
    "                min_entropy = entropy\n",
    "                best_split_feature = column\n",
    "                best_split_value = value\n",
    "    \n",
    "    return best_split_feature, best_split_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_decision_tree(training_data, counter=0, min_samples=2, max_depth=5):\n",
    "    \n",
    "    # converting data which is at first dataframe into 2-d array but after first time it will \n",
    "    # be array, so else case is for that.\n",
    "    if counter == 0:\n",
    "        global COLUMN_HEADERS\n",
    "        COLUMN_HEADERS = training_data.columns\n",
    "        data = training_data.values\n",
    "    else:\n",
    "        data = training_data           \n",
    "    \n",
    "    \n",
    "    # base cases\n",
    "    # 1. if the data has only one label it means it is the base case because there is no need to further split the data\n",
    "    # 2. if counter which is incremented on every recursive call reaches the max_depth\n",
    "    # 3. if datapoints become less than min_samples\n",
    "    if (check_singularity(data)) or (counter == max_depth) or (len(data) < min_samples):\n",
    "        label = classify_data(data)\n",
    "        \n",
    "        return label\n",
    "\n",
    "    \n",
    "    # recursive part\n",
    "    else:    \n",
    "        counter = counter + 1\n",
    "\n",
    "        # getting all splits\n",
    "        splits = get_splits(data)\n",
    "        # finding best split_column and split value\n",
    "        split_column, split_value = find_best_split(data, splits)\n",
    "        #getting data below and data above that split\n",
    "        data_below, data_above = split_data(data, split_column, split_value)\n",
    "        \n",
    "        # instantiate sub-tree\n",
    "        feature_name = COLUMN_HEADERS[split_column]\n",
    "        question = \"{} <= {}\".format(feature_name, split_value)\n",
    "        sub_tree = {question: []}\n",
    "        \n",
    "        # find answers (recursion)\n",
    "        is_classified = build_decision_tree(data_below, counter, min_samples, max_depth)\n",
    "        is_not_classified = build_decision_tree(data_above, counter, min_samples, max_depth)\n",
    "        \n",
    "        # If both is_classified and is_not_classified are same there is no need to further split and break tree\n",
    "        # the data is classified\n",
    "        # yet (min_samples or max_depth base cases).\n",
    "        if is_classified == is_not_classified:\n",
    "            sub_tree = is_classified\n",
    "        else:\n",
    "            sub_tree[question].append(is_classified)\n",
    "            sub_tree[question].append(is_not_classified)\n",
    "        \n",
    "        return sub_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PetalWidth <= 0.8': ['Iris-setosa',\n",
      "                       {'PetalWidth <= 1.75': [{'PetalLength <= 5.05': [{'SepalLength <= 4.95': ['Iris-virginica',\n",
      "                                                                                                 'Iris-versicolor']},\n",
      "                                                                        {'SepalLength <= 6.05': ['Iris-versicolor',\n",
      "                                                                                                 'Iris-virginica']}]},\n",
      "                                               {'PetalLength <= 4.85': [{'SepalWidth <= 3.1': ['Iris-virginica',\n",
      "                                                                                               'Iris-versicolor']},\n",
      "                                                                        'Iris-virginica']}]}]}\n"
     ]
    }
   ],
   "source": [
    "tree = build_decision_tree(training_data, max_depth=5)\n",
    "pprint(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PetalWidth <= 0.8'"
      ]
     },
     "execution_count": 627,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(tree.keys())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(datapoint, tree):\n",
    "    #getting left side of the tree\n",
    "    question = list(tree.keys())[0]\n",
    "    #spliting data into three parts\n",
    "    feature_name, comparison_operator, value = question.split()\n",
    "\n",
    "    # if datapoint's feature name is less than equal to the value then assign label\n",
    "    if datapoint[feature_name] <= float(value):\n",
    "        answer = tree[question][0]\n",
    "    #else assign right part of tree\n",
    "    else:\n",
    "        answer = tree[question][1]\n",
    "\n",
    "    # base case\n",
    "    if not isinstance(answer, dict):\n",
    "        return answer\n",
    "    \n",
    "    #recusrsively going through the tree\n",
    "    else:\n",
    "        residual_tree = answer\n",
    "        return predict(datapoint, residual_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SepalLength               6.7\n",
      "SepalWidth                2.5\n",
      "PetalLength               5.8\n",
      "PetalWidth                1.8\n",
      "class          Iris-virginica\n",
      "Name: 107, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Iris-virginica'"
      ]
     },
     "execution_count": 629,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(testing_data.iloc[1])\n",
    "predict(testing_data.iloc[1],tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating accuracy\n",
    "def calculate_accuracy(testing_data, tree):\n",
    "\n",
    "    testing_data[\"classification\"] = testing_data.apply(predict, axis=1, args=(tree,))\n",
    "    testing_data[\"classification_correct\"] = testing_data[\"classification\"] == testing_data[\"class\"]\n",
    "    \n",
    "    accuracy = testing_data[\"classification_correct\"].mean()\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9555555555555556\n"
     ]
    }
   ],
   "source": [
    "print (calculate_accuracy(testing_data,tree))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
